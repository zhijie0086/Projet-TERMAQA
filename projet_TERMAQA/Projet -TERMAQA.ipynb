{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import json\n",
    "import string\n",
    "import gensim.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ouvrir_json(chemin):\n",
    "    f = open(chemin, encoding=\"utf−8\") \n",
    "    toto = json.load(f)\n",
    "    f.close()\n",
    "    return toto\n",
    "def ecrire_json(dataset,chemin):\n",
    "    donnee = json.dumps(dataset, indent =2,ensure_ascii=False) \n",
    "    w = open(chemin, \"w\",encoding = \"utf-8\")\n",
    "    w.write(donnee)\n",
    "    w.close()\n",
    "    print(\"Jeu de données stocké dans %s\"%chemin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "\ts_without_punct = s\n",
    "\tfor char in string.punctuation:\n",
    "\t\ts_without_punct = s_without_punct.replace(char, ' ') # on replace les ponctuations par des espaces\n",
    "\ttokens = s_without_punct.split()\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(corpus_path):\n",
    "\t# assume there's one sentence per line, tokens separated by whitespace\n",
    "\t# simple_preprocess: \"Convert a document into a list of lowercase tokens, \n",
    "\t# ignoring tokens that are too short or too long.\"\"\n",
    "\twith open(corpus_path, mode='r', encoding='utf-8') as inFile:\n",
    "\t\treturn [preprocess(line) for line in inFile.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(object):\n",
    "\t\"\"\"Programs to learn, save and load embeddings.\"\"\"\n",
    "\tdef __init__(self, corpus_path, model_path):\n",
    "\t\tself._corpus_path = corpus_path\n",
    "\t\tself._model_path = model_path\n",
    "\n",
    "\tdef learn(self):\n",
    "\t\tsentences = get_corpus(self._corpus_path) # List of sentences (a sentence = list of words)\n",
    "\t\tself._model = gensim.models.Word2Vec(sentences=sentences)\n",
    "\t\tself._model.save(self._model_path)\n",
    "\n",
    "\tdef learn_restrictive(self, min_freq):\n",
    "\t\tsentences = get_corpus(self._corpus_path) # List of sentences (a sentence = list of words)\n",
    "\t\tself._model = gensim.models.Word2Vec(sentences=sentences, size=100,window= 20, min_count=min_freq, workers=4)\n",
    "\t\tself._model.save(self._model_path)\n",
    "\n",
    "\tdef load(self):\n",
    "\t\tself._model = gensim.models.Word2Vec.load(self._model_path)\n",
    "\n",
    "\tdef most_similar(self, word, topN=10):\n",
    "\t\treturn self._model.wv.most_similar(positive=[word], topn=topN)\n",
    "\n",
    "\tdef most_similar_analogy(self, pos, neg, topN=10):\n",
    "\t\treturn self._model.wv.most_similar(positive=pos, negative=neg, topn=topN)\n",
    "\n",
    "\tdef similarity(self, w1, w2):\n",
    "\t\tif w1 in self._model.wv and w2 in self._model.wv:\n",
    "\t\t\treturn self._model.wv.similarity(w1, w2)\n",
    "\t\telse:\n",
    "\t\t\treturn -1\n",
    "\n",
    "\tdef get_vocab(self):\n",
    "\t\treturn self._model.wv.vocab\n",
    "\n",
    "\tdef get_vector(self, word):\n",
    "\t\treturn self._model.wv.get_vector(word)\n",
    "\n",
    "\tdef get_model(self):\n",
    "\t\treturn self._model\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_n : corpus \"normal\"(du type faq)\n",
    "#corpus_s corpus \"scientifique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=open(\"output/corpus_n.txt\",\"w\",encoding=\"utf8\")\n",
    "for chemin in  glob.glob('corpus/appariements_faq\\*.*')  :\n",
    "    #print(chemin)\n",
    "    corpus_q_r = ouvrir_json(chemin)\n",
    "    for k,v in corpus_q_r.items():\n",
    "        for l in v :\n",
    "            if l != \" \":\n",
    "                f1.write(l)\n",
    "f1.close( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2=open(\"output/corpus_s.txt\",\"w\",encoding=\"utf8\")\n",
    "\n",
    "for chemin in  glob.glob('corpus/corpus_TALN_2007-2013\\*')  : #il faudra d'aboard décompresser le dossier \"corpus_TALN_2007-2013\"\n",
    "    f = open(chemin,\"r\",encoding=\"utf-8\")\n",
    "    corpus_s = f.read()\n",
    "    f2.write(corpus_s)\n",
    "f.close( )\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subCorpus_path_n = \"output/corpus_n.txt\"\n",
    "model_path_n = \"output/corpus_n.W2Vmodel\"\n",
    "embeddings = Embeddings(subCorpus_path_n, model_path_n)\n",
    "embeddings.learn ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "subCorpus_path_s = \"output/corpus_s.txt\"\n",
    "model_path_s = \"output/corpus_s.W2Vmodel\"\n",
    "embeddings = Embeddings(subCorpus_path_s, model_path_s)\n",
    "embeddings.learn ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_n = Embeddings(subCorpus_path_n, model_path_n)\n",
    "embeddings_n.load( )\n",
    "embeddings_s = Embeddings(subCorpus_path_s, model_path_s)\n",
    "embeddings_s.load( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine_similarity(vec1,vec2):\n",
    "    dot = np.dot(vec1 , vec2)\n",
    "    norm1 = np.linalg.norm(vec1) \n",
    "    norm2 = np.linalg.norm(vec2) \n",
    "    cos = dot / (norm1 * norm2)\n",
    "    return cos # f l o a t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_n, vocab_s = embeddings_n.get_vocab(), embeddings_s.get_vocab()\n",
    "sims = {}\n",
    "for word in vocab_n:\n",
    "    if word in vocab_s:\n",
    "        if word.lower( ) not in stopwords.words(\"french\"):\n",
    "                vec1 = embeddings_n.get_vector(word)\n",
    "                vec2 = embeddings_s.get_vector(word)\n",
    "                sims[word] = cosine_similarity(vec1,vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∗∗∗ Vecteurs proches : \n",
      "régulièrement, photos, publicité, accepter, frais, connaître, paiement, pouvez, dossier, merci, mail, rendez, paris, Message, XXX, visibilité, photo, contact, télécharger, inscription, droits, encore, pièces, tarifs, conseiller, comptable, Non, Merci, envoyé, administrative, ∗∗∗ Vecteurs éloigné :\n",
      "via, puis, selon, alors, ainsi, avant, Si, fait, lors, partir, A, ligne, deux, cours, nouveau, statut, nouvelle, part, première, cas, propositions, proposition, fois, si, où, 1, «, peut, ci, tous, "
     ]
    }
   ],
   "source": [
    "NB_TO_SHOW = 30\n",
    "sims_up = {k:v for k,v in sorted(sims.items (), key=lambda i:i[1], reverse=True )}\n",
    "sims_down = {k:v for k,v in sorted(sims.items (), key=lambda i:i[1])}\n",
    "print(\"∗∗∗ Vecteurs proches : \")\n",
    "i = 0\n",
    "while i < NB_TO_SHOW :\n",
    "    w = list(sims_up.keys ())[i]\n",
    "    print(w,end=\", \")\n",
    "    i += 1\n",
    "\n",
    "print(\"∗∗∗ Vecteurs éloigné :\")\n",
    "i = 0\n",
    "while i < NB_TO_SHOW :\n",
    "          w = list(sims_down .keys ())[i]\n",
    "          print(w, end=\", \")\n",
    "          i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sims_down)\n",
    " #conseiller comtaple droits paiement dossier etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281\n"
     ]
    }
   ],
   "source": [
    "#print(len(vocab_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "droits\tscientifique\tmétiers psychologique Guinaudeau massive témoignages dimensionnalité bruits périphériques Station physiologiques\n"
     ]
    }
   ],
   "source": [
    "word = \"droits\" #mots liés à ce \"terme\"\n",
    "corpus = \"scientifique\"\n",
    "sims = \" \".join([w for w,s in embeddings_s.most_similar(word,topN=10)])\n",
    "print(\"{}\\t{}\\t{}\".format(word,corpus,sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "droits\tnormal\tvotre les mais et dossier première qui vos scolarité du\n"
     ]
    }
   ],
   "source": [
    "word = \"droits\" #mots liés à ce \"terme\"\n",
    "corpus = \"normal\"\n",
    "sims = \" \".join([w for w,s in embeddings_n.most_similar(word,topN=10)])\n",
    "print(\"{}\\t{}\\t{}\".format(word,corpus,sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
